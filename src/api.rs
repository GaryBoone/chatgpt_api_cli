use serde::{Deserialize, Serialize};
use std::collections::HashMap;

// https://platform.openai.com/docs/api-reference/completions/create

#[derive(Debug, Deserialize)]
pub struct Choice {
    // The index number of the choice in the list of choices.
    pub index: u32,
    // The text of the message.
    pub message: Message,
    // The possible values for finish_reason are:
    // - `stop`: API returned complete model output
    // - `length`: Incomplete model output due to max_tokens parameter or token limit
    // - `content_filter`: Omitted content due to a flag from our content filters
    // - `null`: API response still in progress or incomplete. The Serde deserializer will set
    //   a JSON "null" to None if the value is an Option.
    pub finish_reason: Option<String>,
}

#[derive(Debug, Deserialize)]
pub struct Usage {
    //  The number of tokens in the prompt.
    pub prompt_tokens: u32,
    // The number of tokens used to generate the response.
    pub completion_tokens: u32,
    // The total number of tokens used to generate the response.
    pub total_tokens: u32,
}

#[derive(Debug, Deserialize)]
pub struct ChatResponse {
    // The ID of the response.
    pub id: String,
    // The type of response, 'chat.completion' for calls to
    // "https://api.openai.com/v1/chat/completions".
    pub object: String,
    // The creation time of the response, in Unix time.
    pub created: u32,
    // The ID of the model used to generate the response.
    pub model: String,
    // The tokens used counts.
    pub usage: Usage,
    // The chat completion messages.
    pub choices: Vec<Choice>,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct Message {
    // The role of the message. The possible values are: 'user' and 'system', and 'assistant'.
    pub role: String,
    // The text of the message.
    pub content: Option<String>,
}

#[derive(Debug, Default, serde::Serialize)]
pub struct ChatRequest {
    // ID of the model to use. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported.
    pub model: String,

    // The messages to generate chat completions for, in the chat format.
    pub messages: Vec<Message>,

    // What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the
    // output more random, while lower values like 0.2 will make it more focused and deterministic.
    // We generally recommend altering this or top_p but not both. Defaults to 1.0.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f64>,

    // An alternative to sampling with temperature, called nucleus sampling, where the model
    // considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens
    // comprising the top 10% probability mass are considered. We generally recommend altering this
    // or temperature but not both. Defaults to 1.0.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f64>,

    // How many chat completion choices to generate for each input message. Defaults to 1.0
    #[serde(skip_serializing_if = "Option::is_none")]
    pub n: Option<f64>,

    // If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as
    // data-only server-sent events as they become available, with the stream terminated by a data:
    // [DONE] message.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,

    // Up to 4 sequences where the API will stop generating further tokens.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stop: Option<Vec<String>>,

    // The maximum number of tokens allowed for the generated answer. By default, the number of
    // tokens the model can return will be (4,096 - prompt tokens).
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_tokens: Option<u32>,

    // Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
    // in the text so far, increasing the model's likelihood to talk about new topics. Defaults to
    // 0.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub presence_penalty: Option<f64>,

    // Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
    // frequency in the text so far, decreasing the model's likelihood to repeat the same line
    // verbatim. Defaults to 0.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub frequency_penalty: Option<f64>,

    // Modify the likelihood of specified tokens appearing in the completion. Accepts a json object
    // that maps tokens (specified by their token ID in the tokenizer) to an associated bias value
    // from -100 to 100. Mathematically, the bias is added to the logits generated by the model
    // prior to sampling. The exact effect will vary per model, but values between -1 and 1 should
    // decrease or increase likelihood of selection; values like -100 or 100 should result in a ban
    // or exclusive selection of the relevant token.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub logit_bias: Option<HashMap<u32, f64>>,

    // A unique identifier representing your end-user, which can help OpenAI to monitor and detect
    // abuse.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
}
